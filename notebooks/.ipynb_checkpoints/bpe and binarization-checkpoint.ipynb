{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d078edb5",
   "metadata": {},
   "source": [
    "Execution steps are from: https://github.com/pytorch/fairseq/blob/main/examples/bart/README.summarization.md\n",
    "<br/>\n",
    "\n",
    "Refer to the following files for code:\n",
    "1. https://github.com/pytorch/fairseq/blob/main/fairseq/data/encoders/gpt2_bpe_utils.py\n",
    "2. https://github.com/pytorch/fairseq/blob/fcca32258c8e8bcc9f9890bf4714fa2f96b6b3e1/fairseq/binarizer.py#L49\n",
    "3. https://github.com/pytorch/fairseq/blob/fcca32258c8e8bcc9f9890bf4714fa2f96b6b3e1/fairseq/data/dictionary.py#L304\n",
    "4. https://github.com/neulab/guided_summarization/blob/ea4bbe91f189cdb51f7f6a827210f9adc5319b3c/bart/fairseq/models/bart/guided_hub_interface.py#L122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84776b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "595c1221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = (\n",
    "        list(range(ord(\"!\"), ord(\"~\") + 1))\n",
    "        + list(range(ord(\"¡\"), ord(\"¬\") + 1))\n",
    "        + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
    "    )\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2 ** 8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2 ** 8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "399f5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "52f0d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe(token, bpe_ranks, debug=False):\n",
    "    word = tuple(token)\n",
    "    pairs = get_pairs(word)\n",
    "\n",
    "    if not pairs:\n",
    "        return token\n",
    "\n",
    "    while True:\n",
    "        if debug:\n",
    "            print('pairs', pairs)\n",
    "        bigram = min(pairs, key=lambda pair: bpe_ranks.get(pair, float(\"inf\")))\n",
    "        if debug:\n",
    "            print('bigram', bigram)\n",
    "        if bigram not in bpe_ranks:\n",
    "            if debug:\n",
    "                print('{} not in bpe_ranks'.format(bigram))\n",
    "            break\n",
    "        first, second = bigram\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "            \n",
    "            if debug:\n",
    "                print('new_word', new_word)\n",
    "\n",
    "            if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                new_word.append(first + second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "            \n",
    "            if debug:\n",
    "                print('new_word2', new_word)    \n",
    "            \n",
    "        new_word = tuple(new_word)\n",
    "        if debug:\n",
    "            print('outofwhile', new_word)\n",
    "            print()\n",
    "            \n",
    "        word = new_word\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "    word = \" \".join(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b985e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "wget -N https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n",
    "wget -N https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
    "'''\n",
    "\n",
    "encoder_json_path = 'encoder.json'\n",
    "vocab_bpe_path = 'vocab.bpe'\n",
    "\n",
    "with open(encoder_json_path, \"r\") as f:\n",
    "    encoder = json.load(f)\n",
    "decoder = {v: k for k, v in encoder.items()}    \n",
    "\n",
    "with open(vocab_bpe_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n",
    "\n",
    "bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "byte_encoder = bytes_to_unicode()\n",
    "byte_decoder = {v: k for k, v in byte_encoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e7e27141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ġ', 't'), ('Ġ', 'a'), ('h', 'e'), ('i', 'n'), ('r', 'e')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first key appears different from the one in vocab.bpe when opened in browser. Related to encoding probably.\n",
    "bpe_merges[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4e815bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĠShantanu \n",
      "\n",
      "pairs {('Ġ', 'S'), ('S', 'h'), ('a', 'n'), ('n', 't'), ('n', 'u'), ('h', 'a'), ('t', 'a')}\n",
      "bigram ('a', 'n')\n",
      "new_word ['Ġ', 'S', 'h']\n",
      "new_word2 ['Ġ', 'S', 'h', 'an']\n",
      "new_word ['Ġ', 'S', 'h', 'an', 't']\n",
      "new_word2 ['Ġ', 'S', 'h', 'an', 't', 'an']\n",
      "outofwhile ('Ġ', 'S', 'h', 'an', 't', 'an', 'u')\n",
      "\n",
      "pairs {('Ġ', 'S'), ('h', 'an'), ('S', 'h'), ('t', 'an'), ('an', 'u'), ('an', 't')}\n",
      "bigram ('Ġ', 'S')\n",
      "new_word []\n",
      "new_word2 ['ĠS']\n",
      "outofwhile ('ĠS', 'h', 'an', 't', 'an', 'u')\n",
      "\n",
      "pairs {('ĠS', 'h'), ('h', 'an'), ('t', 'an'), ('an', 'u'), ('an', 't')}\n",
      "bigram ('an', 't')\n",
      "new_word ['ĠS', 'h']\n",
      "new_word2 ['ĠS', 'h', 'ant']\n",
      "new_word ['ĠS', 'h', 'ant']\n",
      "new_word2 ['ĠS', 'h', 'ant', 'an']\n",
      "outofwhile ('ĠS', 'h', 'ant', 'an', 'u')\n",
      "\n",
      "pairs {('ĠS', 'h'), ('ant', 'an'), ('h', 'ant'), ('an', 'u')}\n",
      "bigram ('ĠS', 'h')\n",
      "new_word []\n",
      "new_word2 ['ĠSh']\n",
      "outofwhile ('ĠSh', 'ant', 'an', 'u')\n",
      "\n",
      "pairs {('an', 'u'), ('ant', 'an'), ('ĠSh', 'ant')}\n",
      "bigram ('an', 'u')\n",
      "new_word ['ĠSh', 'ant']\n",
      "new_word2 ['ĠSh', 'ant', 'anu']\n",
      "outofwhile ('ĠSh', 'ant', 'anu')\n",
      "\n",
      "pairs {('ant', 'anu'), ('ĠSh', 'ant')}\n",
      "bigram ('ĠSh', 'ant')\n",
      "new_word []\n",
      "new_word2 ['ĠShant']\n",
      "outofwhile ('ĠShant', 'anu')\n",
      "\n",
      "pairs {('ĠShant', 'anu')}\n",
      "bigram ('ĠShant', 'anu')\n",
      "('ĠShant', 'anu') not in bpe_ranks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ĠShant anu'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = ' Shantanu'\n",
    "benc_token = \"\".join(byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
    "print(benc_token, '\\n')\n",
    "bpe(benc_token, bpe_ranks, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "37d1d327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49892, 42357]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = ' Shantanu'\n",
    "benc_token = \"\".join(byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
    "enc_token = [encoder[bpe_token] for bpe_token in bpe(benc_token, bpe_ranks).split(\" \")]\n",
    "enc_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5970a30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĠShantanu\n",
      " Shantanu\n"
     ]
    }
   ],
   "source": [
    "text = \"\".join([decoder.get(token, token) for token in enc_token])\n",
    "print(text)\n",
    "text = bytearray([byte_decoder[c] for c in text]).decode(\n",
    "    \"utf-8\", errors='replace'\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "151bc798",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c497800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', ',', ' I', ' am', ' Shantanu', '.', ' I', \"'ve\", ' been', ' trying', ' understand', ' this', ' code', ' for', ' long', ' now']\n"
     ]
    }
   ],
   "source": [
    "sent = \"Hi, I am Shantanu. I've been trying understand this code for long now\"\n",
    "\n",
    "tokens = re.findall(pat, sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1afb8cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Hi\n",
      ", ,\n",
      " I ĠI\n",
      " am Ġam\n",
      " Shantanu ĠShantanu\n",
      ". .\n",
      " I ĠI\n",
      "'ve 've\n",
      " been Ġbeen\n",
      " trying Ġtrying\n",
      " understand Ġunderstand\n",
      " this Ġthis\n",
      " code Ġcode\n",
      " for Ġfor\n",
      " long Ġlong\n",
      " now Ġnow\n"
     ]
    }
   ],
   "source": [
    "# Space encodede as Gdot. Punctuations remain the same\n",
    "\n",
    "for token in tokens:\n",
    "    benc_token = \"\".join(byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
    "    print(token, benc_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "97ef7279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Hi\n",
      ", ,\n",
      "ĠI ĠI\n",
      "Ġam Ġam\n",
      "ĠShantanu ĠShant anu\n",
      ". .\n",
      "ĠI ĠI\n",
      "'ve 've\n",
      "Ġbeen Ġbeen\n",
      "Ġtrying Ġtrying\n",
      "Ġunderstand Ġunderstand\n",
      "Ġthis Ġthis\n",
      "Ġcode Ġcode\n",
      "Ġfor Ġfor\n",
      "Ġlong Ġlong\n",
      "Ġnow Ġnow\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    benc_token = \"\".join(byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
    "    print(benc_token, bpe(benc_token, bpe_ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "04c21787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17250, 11, 314, 716, 49892, 42357, 13, 314, 1053, 587, 2111, 1833, 428, 2438, 329, 890, 783]\n",
      "Hi, I am Shantanu. I've been trying understand this code for long now\n"
     ]
    }
   ],
   "source": [
    "enc_tokens = []\n",
    "for token in tokens:\n",
    "    benc_token = \"\".join(byte_encoder[b] for b in token.encode(\"utf-8\"))\n",
    "    enc_tokens.extend([encoder[bpe_token] for bpe_token in bpe(benc_token, bpe_ranks).split(\" \")])\n",
    "print(enc_tokens)\n",
    "\n",
    "text = \"\".join([decoder.get(token, token) for token in enc_tokens])\n",
    "text = bytearray([byte_decoder[c] for c in text]).decode(\"utf-8\", errors='replace')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39612a6",
   "metadata": {},
   "source": [
    "## Binarization\n",
    "\n",
    "After we have the byte-pair encoding, we use binarization to convert these ids to a standard one that the encoder is trained on. The mapping is provided by a dict file. We will use this as an example:\n",
    "\n",
    "https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
    "\n",
    "**NOTE**: Refer to this for understanding the flow:\n",
    "https://github.com/neulab/guided_summarization/blob/ea4bbe91f189cdb51f7f6a827210f9adc5319b3c/bart/fairseq/models/bart/guided_hub_interface.py#L84\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1051b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "wget -N https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
    "'''\n",
    "\n",
    "with open('dict.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "    binarization_dict = dict([row.split() for row in f.read().splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4ca38b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('17250', '38737'), ('11', '800251374'), ('314', '60989470'), ('716', '4365322'), ('49892', '14052'), ('42357', '178076'), ('13', '850314647'), ('314', '60989470'), ('1053', '6231567'), ('587', '34803895'), ('2111', '3590156'), ('1833', '1899229'), ('428', '52404829'), ('2438', '768787'), ('329', '155236946'), ('890', '8325716'), ('783', '15823492')]\n"
     ]
    }
   ],
   "source": [
    "in_tokens = list(map(str, enc_tokens))\n",
    "out_tokens = [binarization_dict[token] for token in in_tokens]\n",
    "print(list(zip(in_tokens, out_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e773b085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38737, 800251374, 60989470, 4365322, 14052, 178076, 850314647, 60989470, 6231567, 34803895, 3590156, 1899229, 52404829, 768787, 155236946, 8325716, 15823492]\n"
     ]
    }
   ],
   "source": [
    "print(list(map(int, out_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4afb78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
